{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cb58e7a",
   "metadata": {},
   "source": [
    "# Apexon ChatBot(Success Stories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1826cc",
   "metadata": {},
   "source": [
    "# Extracting Linked URLs\n",
    "Webpage used : https://www.apexon.com/success-stories/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2a7557c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Linked URLs found:\n",
      "Number of Linked URLs found:, 0\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to find all linked pages from the main page\n",
    "def find_linked_pages(url):\n",
    "    # Set up the Selenium WebDriver\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Scroll down to the bottom of the page to load all content (if infinite scrolling is used)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "    # Get the page source after it has been fully loaded\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Close the driver\n",
    "    driver.quit()\n",
    "    \n",
    "    # Find all 'a' tags with href attributes\n",
    "    links = soup.find_all('a', href=True)\n",
    "    \n",
    "    # Use a set to collect distinct URLs\n",
    "    linked_pages = set()\n",
    "    for link in links:\n",
    "        href = link['href']\n",
    "        # Only consider URLs that match the pattern\n",
    "        if href.startswith('https://www.apexon.com/resources/case-studies/'):\n",
    "            linked_pages.add(href)\n",
    "    \n",
    "    # Convert the set back to a list for ordered printing\n",
    "    return list(linked_pages)\n",
    "\n",
    "# Main URL to process\n",
    "main_url = \"https://www.apexon.com/success-stories/\"\n",
    "\n",
    "# Extract and print linked URLs\n",
    "linked_urls = find_linked_pages(main_url)\n",
    "x = 0\n",
    "print(\"Distinct Linked URLs found:\")\n",
    "for url in linked_urls:\n",
    "    print(url)\n",
    "    x = x+1\n",
    "print(f\"Number of Linked URLs found:, {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f559ce22",
   "metadata": {},
   "source": [
    "# Storing webpage Linked URLs text and embeddings in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4bd9db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing main URL: https://www.apexon.com/success-stories/\n",
      "Data has been saved to extracted_data_and_embeddings.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import openai\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Function to extract text from a webpage\n",
    "def extract_text_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Remove script and style elements\n",
    "    for script_or_style in soup(['script', 'style']):\n",
    "        script_or_style.decompose()\n",
    "    \n",
    "    # Extract and return the cleaned text content\n",
    "    text = soup.get_text(separator=' ')\n",
    "    return text\n",
    "\n",
    "# Function to split text into sections\n",
    "def split_text_into_sections(text, max_length=1000):\n",
    "    words = text.split()\n",
    "    sections = []\n",
    "    current_section = []\n",
    "    current_length = 0\n",
    "    for word in words:\n",
    "        current_length += len(word) + 1\n",
    "        if current_length > max_length:\n",
    "            sections.append(' '.join(current_section))\n",
    "            current_section = []\n",
    "            current_length = len(word) + 1\n",
    "        current_section.append(word)\n",
    "    if current_section:\n",
    "        sections.append(' '.join(current_section))\n",
    "    return sections\n",
    "\n",
    "# Function to convert text sections into OpenAI embeddings using batching\n",
    "def get_openai_embeddings_batch(sections):\n",
    "    response = openai.Embedding.create(\n",
    "        input=sections,\n",
    "        model=\"text-embedding-ada-002\"  # Use the correct model name\n",
    "    )\n",
    "    return [item['embedding'] for item in response['data']]\n",
    "\n",
    "# Setting OpenAI API key\n",
    "openai.api_key = 'sk-None-dvpc3JgnrJeJTsYUkTs9T3BlbkFJaLbQpougGlLcVMLDtRrI'  # Ensure this is correct\n",
    "\n",
    "# Function to process a single URL\n",
    "def process_url(url):\n",
    "    text = extract_text_from_url(url)\n",
    "    sections = split_text_into_sections(text)\n",
    "    embeddings = get_openai_embeddings_batch(sections)\n",
    "    return url, sections, embeddings\n",
    "\n",
    "# Main URL to process\n",
    "main_url = \"https://www.apexon.com/success-stories/\"\n",
    "\n",
    "# Process the main URL\n",
    "print(f\"Processing main URL: {main_url}\")\n",
    "main_url_text = extract_text_from_url(main_url)\n",
    "main_url_sections = split_text_into_sections(main_url_text)\n",
    "main_url_embeddings = get_openai_embeddings_batch(main_url_sections)\n",
    "\n",
    "# Dictionary to store all embeddings and sections\n",
    "data_to_save = []\n",
    "\n",
    "# Add main URL data to the list\n",
    "for section, embedding in zip(main_url_sections, main_url_embeddings):\n",
    "    data_to_save.append({\"URL\": main_url, \"Text\": section, \"Embedding\": embedding})\n",
    "\n",
    "# Sample list of linked URLs (replace with actual URLs)\n",
    "\n",
    "# Use ThreadPoolExecutor to process URLs concurrently\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = {executor.submit(process_url, url): url for url in linked_urls}\n",
    "    for future in as_completed(futures):\n",
    "        url, sections, embeddings = future.result()\n",
    "        for section, embedding in zip(sections, embeddings):\n",
    "            data_to_save.append({\"URL\": url, \"Text\": section, \"Embedding\": embedding})\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df = pd.DataFrame(data_to_save)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel(\"extracted_data_and_embeddings.xlsx\", index=False)\n",
    "\n",
    "print(\"Data has been saved to extracted_data_and_embeddings.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5821622e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
